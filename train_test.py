# -*- coding: utf-8 -*-
"""VideoDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ulHXWiVIKU_9vEbl7p3_7cs0Z1y54W8M
"""

import torch
import os
import time
import copy
import numpy as np
import torchvision
from torch.utils.data import Dataset, SubsetRandomSampler
from torchvision import datasets
import pandas as pd

from torchvision.transforms import Compose, Lambda, RandomHorizontalFlip
from torchvision.transforms._transforms_video import (
    CenterCropVideo,
    NormalizeVideo,
    RandomCrop

)

from pytorchvideo.data.encoded_video import EncodedVideo
from pytorchvideo.transforms import (
    ApplyTransformToKey,
    UniformTemporalSubsample,
    RandomShortSideScale
)
from typing import Dict

import torch.optim as optim
from torch import nn
from pathlib import Path
from video_dataset import VideoFrameDataset, ImglistToTensor

'''
####################
# SlowFast transform
####################

side_size = 224
mean = [0.45, 0.45, 0.45]
std = [0.225, 0.225, 0.225]
crop_size = 224
# num_frames = 32
num_frames = 8
sampling_rate = 2
frames_per_second = 30
alpha = 4

class PackPathway(torch.nn.Module):
    """
    Transform for converting video frames as a list of tensors.
    """
    def __init__(self):
        super().__init__()

    def forward(self, frames: torch.Tensor):
        fast_pathway = frames
        # Perform temporal sampling from the fast pathway.
        slow_pathway = torch.index_select(
            frames,
            1,
            torch.linspace(
                0, frames.shape[1] - 1, frames.shape[1] // alpha
            ).long(),
        )
        frame_list = [slow_pathway, fast_pathway]
        return frame_list

transform =  ApplyTransformToKey(
    key="video",
    transform=Compose(
        [
            UniformTemporalSubsample(num_frames),
            Lambda(lambda x: x/255.0),
            NormalizeVideo(mean, std),
            RandomShortSideScale(min_size=256, max_size=320),
            RandomCrop(224),
            RandomHorizontalFlip(p=0.5)
        ]
    ),
)
'''
model_path = Path("models")
model_path.mkdir(parents=True, exist_ok=True)

model_name = "slow_r50_deadlift_v1.pth"
model_save_path = model_path / model_name


####################
# Slow transform
####################

side_size = 224
mean = [0.45, 0.45, 0.45]
std = [0.225, 0.225, 0.225]
crop_size = 224
num_frames = 8
sampling_rate = 8
frames_per_second = 30

# Note that this transform is specific to the slow_R50 model.
transform = ApplyTransformToKey(
    key="video",
    transform=Compose(
        [
            UniformTemporalSubsample(num_frames),
            Lambda(lambda x: x / 255.0),
            NormalizeVideo(mean, std),
            RandomShortSideScale(min_size=256, max_size=320),
            RandomCrop(224),
            RandomHorizontalFlip(p=0.5)
        ]
    ),
)

# The duration of the input clip is also specific to the model.
clip_duration = (num_frames * sampling_rate) / frames_per_second

'''
class DeadliftDataset(Dataset):
    def __init__(self, csv_file, **kwargs):
        # super().__init__(**kwargs)
        self.videos = pd.read_csv(csv_file)
        self.root_dir = kwargs['root']
        self.transform = kwargs['transform']

    def __len__(self):
        return len(self.videos)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        video_path = os.path.join(self.root_dir, self.videos.iloc[idx, 0])

        start_sec = 0
        end_sec = start_sec + clip_duration

        video = EncodedVideo.from_path(video_path)
        video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)

        label = self.videos.iloc[idx, 1]

        if self.transform:
            # print("Transforming ...")
            video_data = self.transform(video_data)
            # print("Transformation done")

        # sample = {'video' : video_data, 'label' : label }

        return video_data["video"], label
    
dataset_path = "Dataset"
# dataset = "Dataset_small"
# deadlift_dataset = DeadliftDataset(root=dataset_path,
#                                    csv_file=dataset_path+"/dataset.csv", transform=transform)

deadlift_dataset  = datasets.VideoLabelDataset(
    dataset_path + "/dataset.csv",
    transform=torchvision.transforms.Compose([
        transforms.VideoFilePathToTensor(max_len=60, fps=25, padding_mode='last'),
        UniformTemporalSubsample(num_frames),
        Lambda(lambda x: x / 255.0),
        NormalizeVideo(mean, std),
        RandomShortSideScale(min_size=256, max_size=320),
        RandomCrop(224),
        RandomHorizontalFlip(p=0.5)

    ])
)
'''
dataset_path = "Dataset_frames"
preprocess = Compose(
        [   ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor
            # transforms.Resize(299),  # image batch, resize smaller edge to 299
            # transforms.CenterCrop(299),
            UniformTemporalSubsample(num_frames),
            Lambda(lambda x: x / 255.0),
            NormalizeVideo(mean, std),
            RandomShortSideScale(min_size=256, max_size=320),
            RandomCrop(224),
            RandomHorizontalFlip(p=0.5)
        ]
    )
deadlift_dataset = VideoFrameDataset(
    root_path = dataset_path,
    annotationfile_path = dataset_path+"/annotations.txt",
    num_segments =4,
    frames_per_segment=5,
    imagefile_template='img_{:05d}.jpg',
    transform=preprocess,
    test_mode=False

)
dataset_size = len(deadlift_dataset)
dataset_indices = list(range(dataset_size))
np.random.shuffle(dataset_indices)
val_split_index = int(np.floor(0.2 * dataset_size))
train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]

train_sampler = SubsetRandomSampler(train_idx)
val_sampler = SubsetRandomSampler(val_idx)

train_loader = torch.utils.data.DataLoader(
    batch_size=2,
    dataset=deadlift_dataset,
    shuffle=True,
    # sampler=train_sampler,
    num_workers=0,
    pin_memory=True

)
val_loader = torch.utils.data.DataLoader(
    dataset=deadlift_dataset,
    batch_size=2,
    shuffle = False,
    num_workers=0,
    pin_memory=True)




device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False


model_name = "slow_r50"
num_classes = 2
# batch_size = 8
num_epochs = 10
feature_extract = True


def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):
    # Initialize these variables which will be set in this if statement. Each of these
    #   variables is model specific.
    model_ft = None
    input_size = 0

    if model_name == "slowfast_r50":
        """ Slowfast_R50 
        """
        model_ft = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.blocks[6].proj.in_features
        model_ft.blocks[6].proj = nn.Linear(num_ftrs, num_classes)
        input_size = 256



    elif model_name == "slow_r50":
        """ Slow_R50
        """
        model_ft = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.blocks[5].proj.in_features
        model_ft.blocks[5].proj = nn.Linear(num_ftrs, num_classes)
        input_size = 224

    else:
        print("Invalid model name, exiting...")
        exit()

    return model_ft, input_size


# Initialize the model for this run
model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)

# Print the model we just instantiated
# print(model_ft)


def train_model(model, dataloader, criterion, optimizer, num_epochs=25, is_inception=False):
    since = time.time()

    val_acc_history = []

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            # for phase in ['train']:
            if phase == 'train':
                print("Train Mode")
                model.train()  # Set model to training mode
            else:
                print("Eval Mode")
                model.eval()  # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            # for inputs, labels in dataloaders[phase]:
            print("Start training ...")
            for inputs, labels in dataloader:
                inputs = inputs.to(device)
                # for id, i in enumerate(inputs):
                # print(f"Frame numero {id}, shape : {i.shape} \n")
                # inputs = [i.to(device)[None, ...] for i in inputs]
                labels = labels.to(device)
                print(f"Shape inputs {inputs.shape}")
                print(f"Shape labels {labels.shape}")
                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    # Get model outputs and calculate loss
                    # Special case for inception because in training it has an auxiliary output. In train
                    #   mode we calculate the loss by summing the final output and the auxiliary output
                    #   but in testing we only consider the final output.
                    '''if is_inception and phase == 'train':
                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958
                        outputs, aux_outputs = model(inputs)
                        loss1 = criterion(outputs, labels)
                        loss2 = criterion(aux_outputs, labels)
                        loss = loss1 + 0.4*loss2
                    else:
                      '''
                    if phase == 'train':
                        print("Forward")
                        outputs = model(inputs)

                        # ouputs = outputs.unsqueeze(1)

                        loss = criterion(outputs, labels)

                    _, preds = torch.max(outputs, 1)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloader[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloader[phase].dataset)

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

            # deep copy the model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
            if phase == 'val':
                val_acc_history.append(epoch_acc)

        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model, val_acc_history


def train_model_v2(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):
    since = time.time()
    val_acc_history = []

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print(f"Epoch : {epoch} of {num_epochs}")

        # Each epoch has a training and validation phase

        model.train()
        running_loss = 0.0
        running_corrects = 0
        for inputs, labels in train_loader:
            print(f"New training batch -> Inputs shape : {inputs.shape}, labels shape : {labels.shape}")
            inputs = inputs.to(device)
            labels = labels.to(device)
            # print(f"Shape inputs {inputs.shape}")
            # print(f"Shape labels {labels.shape}")
            with torch.set_grad_enabled(True):
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                print("Gradient descent done")
                _, preds = torch.max(outputs, 1)
                print(f"Preds : {preds}")
                # statistics

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
                print("Loss e acc added to the epoch stats")

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = running_corrects.double() / len(train_loader.dataset)

        print(f"Epoch loss (mean): {epoch_loss}, Epoch acc : {epoch_acc}")

        # Evaluation
        model.eval()
        running_loss_val = 0.0
        running_corrects_val = 0
        with torch.inference_mode():
            for inputs, labels in val_loader:
                print(f"New evaluation batch -> Inputs shape : {inputs.shape}, labels shape : {labels.shape}")
                inputs = inputs.to(device)
                labels = labels.to(device)
                outputs = model(inputs)
                loss_val = criterion(outputs, labels)
                running_loss_val += loss_val.item() * inputs.size(0)
                _, preds_val = torch.max(outputs, 1)
                running_corrects_val += torch.sum(preds_val == labels)

            epoch_loss_val = running_loss_val / len(val_loader.dataset)
            epoch_acc_val = running_corrects_val / len(val_loader.dataset)
            if epoch_acc_val > best_acc:
                best_acc = epoch_acc_val
                best_models_wts = copy.deepcopy(model.state_dict())
            val_acc_history.append(epoch_acc_val)

            # deep copy the model

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model, val_acc_history

model_ft = model_ft.to(device)
# Gather the parameters to be optimized/updated in this run. If we are
#  finetuning we will be updating all parameters. However, if we are
#  doing feature extract method, we will only update the parameters
#  that we have just initialized, i.e. the parameters with requires_grad
#  is True.
params_to_update = model_ft.parameters()
print("Params to learn:")
if feature_extract:
    params_to_update = []
    for name, param in model_ft.named_parameters():
        if param.requires_grad == True:
            params_to_update.append(param)
            print("\t", name)
else:
    for name, param in model_ft.named_parameters():
        if param.requires_grad == True:
            print("\t", name)

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)

loss = nn.CrossEntropyLoss()
model_ft = model_ft.to(device)
model_ft, hist = train_model_v2(model_ft, train_loader, val_loader, loss, optimizer_ft, num_epochs=num_epochs)
print(f"Saving model to; {model_save_path}")
torch.save(obj=model_ft.state_dict(), f=model_save_path)

# Example of loading saved model parameters




'''
# Load the example video
video_path = "/content/gdrive/MyDrive/Dataset/Good/000002.mp4"

# Select the duration of the clip to load by specifying the start and end duration
# The start_sec should correspond to where the action occurs in the video
start_sec = 0
end_sec = start_sec + clip_duration

# Initialize an EncodedVideo helper class
video = EncodedVideo.from_path(video_path)

# Load the desired clip
video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)

# Apply a transform to normalize the video input
video_data = transform(video_data)
# Move the inputs to the desired device
inputs = video_data["video"]
inputs = [i.to(device)[None, ...] for i in inputs]
'''