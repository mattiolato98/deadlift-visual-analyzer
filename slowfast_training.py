# -*- coding: utf-8 -*-
"""Train_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jH2c90fPxl6MGxA5TNuSACIolpml4vd9
"""
import torch
import os
import time
import copy
import numpy as np
from torchvision import datasets
import pandas as pd

from torchvision.transforms import Compose, Lambda, RandomHorizontalFlip
from torchvision.transforms._transforms_video import (
    CenterCropVideo,
    NormalizeVideo,
    CenterCropVideo,
    RandomCrop

)

from pytorchvideo.data.encoded_video import EncodedVideo
from pytorchvideo.transforms import (
    ApplyTransformToKey,
    UniformTemporalSubsample,
    RandomShortSideScale,
    ShortSideScale
)
from typing import Dict

import torch.optim as optim
from torch import nn
from pathlib import Path
import matplotlib.pyplot as plt
from torch.utils.data import RandomSampler, SequentialSampler
import pickle
####################
# SlowFast transform
####################

side_size = 256
mean = [0.45, 0.45, 0.45]
std = [0.225, 0.225, 0.225]
crop_size = 256
num_frames = 32
# num_frames =8
sampling_rate = 2
frames_per_second = 30
alpha = 4

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class PackPathway(torch.nn.Module):
    """
    Transform for converting video frames as a list of tensors.
    """

    def __init__(self):
        super().__init__()

    def forward(self, frames: torch.Tensor):
        fast_pathway = frames
        # Perform temporal sampling from the fast pathway.
        slow_pathway = torch.index_select(
            frames,
            1,
            torch.linspace(
                0, frames.shape[1] - 1, frames.shape[1] // alpha
            ).long(),
        )
        frame_list = [slow_pathway, fast_pathway]
        return frame_list


transform = ApplyTransformToKey(
    key="video",
    transform=Compose(
        [
            UniformTemporalSubsample(num_frames),
            Lambda(lambda x: x / 255.0),
            NormalizeVideo(mean, std),
            RandomShortSideScale(min_size=256, max_size=320),
            RandomCrop(224),
            RandomHorizontalFlip(p=0.5),
            PackPathway()
        ]
    ),
)
'''
####################
# Slow transform
####################

side_size = 224
mean = [0.45, 0.45, 0.45]
std = [0.225, 0.225, 0.225]
crop_size = 224
num_frames = 8
sampling_rate = 8
frames_per_second = 30

# Note that this transform is specific to the slow_R50 model.
transform =  ApplyTransformToKey(
    key="video",
    transform=Compose(
        [
            UniformTemporalSubsample(num_frames),
            Lambda(lambda x: x/255.0),
            NormalizeVideo(mean, std),
            RandomShortSideScale(min_size=256, max_size=320),
            RandomCrop(224),
            RandomHorizontalFlip(p=0.5)
        ]
    ),
)
'''
# The duration of the input clip is also specific to the model.
clip_duration = (num_frames * sampling_rate) / frames_per_second


class DeadliftDataset(datasets.VisionDataset):
    def __init__(self, csv_file, **kwargs):
        super().__init__(**kwargs)
        self.videos = pd.read_csv(csv_file)
        self.root_dir = kwargs['root']
        self.transform = kwargs['transform']

    def __len__(self):
        return len(self.videos)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        video_path = os.path.join(self.root_dir, self.videos.iloc[idx, 0])

        start_sec = 0
        end_sec = start_sec + clip_duration

        video = EncodedVideo.from_path(video_path)
        video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)

        label = self.videos.iloc[idx, 1]
        if self.transform:
            # print("Transforming ...")
            video_data = self.transform(video_data)
            # print("Transformation done")

        # sample = {'video' : video_data, 'label' : label }

        return video_data["video"], label


def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False


entrypoints = torch.hub.list('facebookresearch/pytorchvideo', force_reload=True)
for model in entrypoints:
    print(model)


def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):
    # Initialize these variables which will be set in this if statement. Each of these
    # variables is model specific.
    model_ft = None
    input_size = 0

    if model_name == "slowfast_r50":
        """ Slowfast_R50 
        """
        model_ft = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.blocks[6].proj.in_features
        model_ft.blocks[6].proj = nn.Linear(num_ftrs, num_classes)
        input_size = 256



    elif model_name == "slow_r50":
        """ Slow_R50
        """
        model_ft = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.blocks[5].proj.in_features
        model_ft.blocks[5].proj = nn.Linear(num_ftrs, num_classes)
        input_size = 224


    elif model_name == "slowfast_r101":
        model_ft = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.blocks[6].proj.in_features
        model_ft.blocks[6].proj = nn.Linear(num_ftrs, num_classes)
        input_size = 224

    else:
        print("Invalid model name, exiting...")
        exit()

    return model_ft, input_size


def train_model_v2(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):
    since = time.time()
    val_acc_history = []
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print(f"Epoch : {epoch} of {num_epochs}")

        # Each epoch has a training and validation phase

        model.train()
        running_loss = 0.0
        running_corrects = 0
        for inputs, labels in train_loader:
            # print(f"New training batch -> Inputs shape : {inputs.shape}, labels shape : {labels.shape}")
            # inputs = inputs.to(device)
            inputs = [i.to(device) for i in inputs]
            labels = labels.to(device)
            # print(f"Shape inputs {inputs.shape}")
            # print(f"Shape labels {labels.shape}")
            with torch.set_grad_enabled(True):
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                _, preds = torch.max(outputs, 1)
                # print(f"Preds : {preds}")
                # statistics

                running_loss += loss.item() * inputs[0].size(0)
                running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = running_corrects.double() / len(train_loader.dataset)

        print(f"Epoch loss (mean): {epoch_loss}, Epoch acc : {epoch_acc}")

        # Evaluation
        print(f"Evaluation of epoch {epoch} of {num_epochs}")
        model.eval()
        running_loss_val = 0.0
        running_corrects_val = 0
        with torch.inference_mode():
            for inputs, labels in val_loader:
                # print(f"New evaluation batch -> Inputs shape : {inputs.shape}, labels shape : {labels.shape}")
                # inputs = inputs.to(device)
                inputs = [i.to(device) for i in inputs]
                labels = labels.to(device)
                outputs = model(inputs)
                loss_val = criterion(outputs, labels)

                running_loss_val += loss_val.item() * inputs[0].size(0)
                _, preds_val = torch.max(outputs, 1)
                print(f"Real labels : {labels} ")
                print(f"Predictions : {preds_val} \n")
                running_corrects_val += torch.sum(preds_val == labels)

            epoch_loss_val = running_loss_val / len(val_loader.dataset)
            epoch_acc_val = running_corrects_val / len(val_loader.dataset)
            print(f"Epoch val loss (mean): {epoch_loss_val} item {loss_val.item()}, Epoch val acc : {epoch_acc_val}")

            if epoch_acc_val > best_acc:
                best_acc = epoch_acc_val
                best_models_wts = copy.deepcopy(model.state_dict())
            val_acc_history.append(epoch_acc_val)

            train_loss_values.append(epoch_loss)
            val_loss_values.append(epoch_loss_val)
            epoch_count.append(epoch)

            # deep copy the model

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)

    print(f"Saving model parameters to {model_save_path}")
    torch.save(obj=model_ft.state_dict(), f=model_save_path)

    return model, val_acc_history


def split_dataset(dataset_name, batch_size=8,transform=None ):
    dataset = os.getcwd() + f"/{dataset_name}"
    deadlift_dataset = DeadliftDataset(root=dataset, csv_file=dataset + "/dataset.csv", transform=transform)

    dataset_size = len(deadlift_dataset)


    '''
    dataset_indices = list(range(dataset_size))
    np.random.shuffle(dataset_indices)
    val_split_index = int(np.floor(0.2 * dataset_size))
    train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]
    
    train_sampler = SubsetRandomSampler(train_idx)
    # val_sampler = SubsetRandomSampler(val_idx)
    val_sampler = SequentialSampler(val_idx)
    '''
    val_size = int(np.floor(0.2 * dataset_size))
    train_size = dataset_size - val_size
    train_set, val_set = torch.utils.data.random_split(deadlift_dataset, [train_size, val_size],
                                                       generator=torch.Generator().manual_seed(42))
    train_sampler = RandomSampler(train_set, replacement=True)

    train_loader = torch.utils.data.DataLoader(
        batch_size=batch_size,
        dataset=train_set,
        sampler=train_sampler,
        num_workers=0,
        pin_memory=True

    )
    val_loader = torch.utils.data.DataLoader(
        dataset=val_set,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True)

    return train_loader, val_loader


def model_evaluation(train_loss_values, val_loss_values, model_name, num_classes, feature_extract, model_save_path):
    # Load best model's parameters
    model_v1, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)
    model_v1.load_state_dict(torch.load(model_save_path))

    print(num_epochs)
    print(torch.tensor(train_loss_values).shape)

    '''
    plt.plot(epoch_count, torch.tensor(train_loss_values).numpy(), label="Train loss")
    plt.plot(epoch_count, torch.tensor(val_loss_values).numpy(), label="Val loss")
    plt.title("Training and Test loss curves")
    plt.ylabel("Loss")
    plt.xlabel("Epochs")
    plt.legend();
    '''


    path = os.getcwd()+f"/Deadlift_models/{saving_model_name}.pickle"
    save_object = (train_loss_values, val_loss_values)
    with open(path, 'wb') as handle:
        pickle.dump(save_object, handle, protocol=pickle.HIGHEST_PROTOCOL)


    # with open('filename.pickle', 'rb') as handle:
    #     b = pickle.load(handle)


def inference():
    test_transform = ApplyTransformToKey(
        key="video",
        transform=Compose(
            [
                UniformTemporalSubsample(num_frames),
                Lambda(lambda x: x / 255.0),
                NormalizeVideo(mean, std),
                ShortSideScale(
                    size=side_size
                ),
                CenterCropVideo(crop_size)
            ]
        ),
    )
    # Load the example video
    video_path = "/content/gdrive/MyDrive/Dataset/Test/02.mp4"

    # Select the duration of the clip to load by specifying the start and end duration
    # The start_sec should correspond to where the action occurs in the video
    start_sec = 0
    end_sec = start_sec + clip_duration

    # Initialize an EncodedVideo helper class
    video = EncodedVideo.from_path(video_path)

    # Load the desired clip
    video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)

    # Apply a transform to normalize the video input
    video_data = test_transform(video_data)
    # Move the inputs to the desired device
    inputs = video_data["video"]
    print(inputs.shape)
    inputs = [i.to(device) for i in inputs]
    # inputs = inputs.unsqueeze(0)
    model_ft.eval()
    post_act = torch.nn.Sigmoid()
    preds = model_ft(inputs)
    preds = post_act(preds)
    pred_classes = preds.topk(k=1).indices
    if pred_classes.item() == 0:
        pred_label = "Bad"
    else:
        pred_label = "Good"

    print(f"Pred : {pred_classes}, {pred_label} ")

    test_transform = ApplyTransformToKey(
        key="video",
        transform=Compose(
            [
                UniformTemporalSubsample(num_frames),
                Lambda(lambda x: x / 255.0),
                NormalizeVideo(mean, std),
                ShortSideScale(
                    size=side_size
                ),
                CenterCropVideo(crop_size)
            ]
        ),
    )
    # Load the example video
    video_path = ""
    video = EncodedVideo.from_path(video_path)
    predictions = torch.empty(0)
    for start_sec, end_sec in timestamp_list:
        # Load the desired clip
        video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)

        # Apply a transform to normalize the video input
        video_data = test_transform(video_data)
        # Move the inputs to the desired device
        inputs = video_data["video"]
        inputs = [i.to(device) for i in inputs]

        model_ft.eval()
        post_act = torch.nn.Sigmoid()
        preds = model_ft(inputs)
        preds = post_act(preds)
        pred_classes = preds.topk(k=1).indices
        predictions.add(pred_classes.item())


if __name__ == '__main__':

    model_name = "slowfast_r101"
    num_classes = 2

    num_epochs = 200
    feature_extract = True
    # Initialize the model for this run
    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)

    # Print the model we just instantiated
    # print(model_ft)


    # Gather the parameters to be optimized/updated in this run. If we are
    #  finetuning we will be updating all parameters. However, if we are
    #  doing feature extract method, we will only update the parameters
    #  that we have just initialized, i.e. the parameters with requires_grad
    #  is True.
    params_to_update = model_ft.parameters()
    print("Params to learn:")
    if feature_extract:
        params_to_update = []
        for name, param in model_ft.named_parameters():
            if param.requires_grad == True:
                params_to_update.append(param)
                print("\t", name)
    else:
        for name, param in model_ft.named_parameters():
            if param.requires_grad == True:
                print("\t", name)

    # Observe that all parameters are being optimized
    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)

    saving_model_name = "slowfast_r101_v1_200epochs_final"
    train_loss_values = []
    val_loss_values = []
    epoch_count = []

    model_path = Path(os.getcwd()+"/Deadlift_models/")
    model_path.mkdir(parents=True, exist_ok=True)
    model_name = f"{saving_model_name}.pth"
    model_save_path = model_path / model_name

    dataset = "Dataset_downscaled_720p"

    train_loader, val_loader = split_dataset(dataset, batch_size=16, transform=transform)

    loss_fn = nn.CrossEntropyLoss()
    model_ft = model_ft.to(device)
    model_ft, hist = train_model_v2(model_ft, train_loader, val_loader, loss_fn, optimizer_ft, num_epochs=num_epochs)

    model_evaluation(train_loss_values, val_loss_values, model_name, num_classes, feature_extract, model_save_path)

    # inference()
